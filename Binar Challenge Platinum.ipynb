{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "#For Flask\n",
    "from flask import Flask, jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rahad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rahad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Import library for tokeize, stemming, and stopwords\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as stopwords_scratch\n",
    "\n",
    "#Import library for sklearn model sentiment analysis\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library for Tensorflow Model Sentiment Analysis\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow import keras\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "     ------------------------------------ 235.9/235.9 kB 206.2 kB/s eta 0:00:00\n",
      "Installing collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\rahad\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# Import library for Flask\n",
    "from flask import Flask, request, jsonify,render_template\n",
    "from flasgger import Swagger, LazyString, LazyJSONEncoder, swag_from\n",
    "!pip install unidecode\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rahad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator LabelBinarizer from version 0.23.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\rahad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator MLPClassifier from version 0.23.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\rahad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.23.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "c:\\Users\\rahad\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:329: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.23.2 when using version 1.1.3. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Swagger UI Definition\n",
    "app = Flask(__name__)\n",
    "\n",
    "app.json_encoder = LazyJSONEncoder\n",
    "swagger_template = dict(\n",
    "info = {\n",
    "    'title' : LazyString(lambda: 'Sental (Sentiment Analysis)'),\n",
    "    'version' : LazyString(lambda : '1.0.0'),\n",
    "    'description': LazyString(lambda : 'Data menganalisa suatu sentiment'),\n",
    "},\n",
    "    host = LazyString(lambda: request.host)\n",
    ")\n",
    "\n",
    "swagger_config = {\n",
    "    'headers': [],\n",
    "    'specs': [\n",
    "        {\n",
    "        'endpoint': 'docs',\n",
    "        'route': '/docs.json',\n",
    "        }\n",
    "    ],\n",
    "    'static_url_path': '/flasgger_static',\n",
    "    'swagger_ui': True,\n",
    "    'specs_route': '/docs/'\n",
    "}\n",
    "swagger = Swagger(app, template=swagger_template,\n",
    "                 config = swagger_config)\n",
    "\n",
    "#Connect db and csv\n",
    "conn = sqlite3.connect('data/output.db', check_same_thread=False)\n",
    "df_alay = pd.read_csv('data/new_kamusalay.csv', names=['alay', 'cleaned'], encoding= 'latin-1')\n",
    "df_raw = pd.read_csv('data/train_preprocess.tsv', sep='\\t', names=['Text', 'Sentiment'])\n",
    "df_raw.drop_duplicates()\n",
    "\n",
    "#Define and execute query for unexistence data tables\n",
    "#Tables will contain fields with dirty text (text & file) and cleaned text (text & file)\n",
    "conn.execute('''CREATE TABLE IF NOT EXISTS data_text_sk (text_id INTEGER PRIMARY KEY AUTOINCREMENT, Text varchar(255), Sentiment varchar(255));''')\n",
    "conn.execute('''CREATE TABLE IF NOT EXISTS data_file_sk (text_id INTEGER PRIMARY KEY AUTOINCREMENT, Text varchar(255), Sentiment varchar(255));''')\n",
    "conn.execute('''CREATE TABLE IF NOT EXISTS data_text_tf (text_id INTEGER PRIMARY KEY AUTOINCREMENT, Text varchar(255), Sentiment varchar(255));''')\n",
    "conn.execute('''CREATE TABLE IF NOT EXISTS data_file_tf (text_id INTEGER PRIMARY KEY AUTOINCREMENT, Text varchar(255), Sentiment varchar(255));''')\n",
    "\n",
    "list_stopwords = stopwords_scratch.words('indonesian')\n",
    "list_stopwords_en = stopwords_scratch.words('english')\n",
    "list_stopwords.extend(list_stopwords_en)\n",
    "list_stopwords.extend(['ya', 'yg', 'ga', 'yuk', 'dah', 'baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'])\n",
    "\n",
    "#Add External Stopwords\n",
    "f = open(\"stopwords/tala-stopwords-indonesia.txt\", \"r\")\n",
    "stopword_external = []\n",
    "for line in f:\n",
    "    stripped_line = line.strip()\n",
    "    line_list = stripped_line.split()\n",
    "    stopword_external.append(line_list[0])\n",
    "f.close()\n",
    "list_stopwords.extend(stopword_external)\n",
    "stopwords = list_stopwords\n",
    "\n",
    "#Creating function for Cleansing Process\n",
    "def lowercase(text): # Change uppercase characters to lowercase\n",
    "    return text.lower()\n",
    "\n",
    "def special(text):\n",
    "    text = re.sub(r'\\W', ' ', str(text), flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def single(text):\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def singlestart(text):\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def mulspace(text):\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "#Removing RT\n",
    "def rt(text):\n",
    "    text = re.sub(r'rt @\\w+: ', ' ', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "#Removing prefixed 'b'\n",
    "def prefixedb(text):\n",
    "    text = re.sub(r'^b\\s+', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def misc(text):\n",
    "    text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))|([#@]\\S+)|user|\\n|\\t', ' ', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "#Mapping for kamusalay\n",
    "alay_mapping = dict(zip(df_alay['alay'], df_alay['cleaned']))\n",
    "def alay(text):\n",
    "    wordlist = text.split()\n",
    "    text_alay = [alay_mapping.get(x,x) for x in wordlist]\n",
    "    clean_alay = ' '.join(text_alay)\n",
    "    return clean_alay\n",
    "\n",
    "def stopwrds(text): \n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords]\n",
    "    output_sw = ' '.join(tokens_without_sw)\n",
    "    return output_sw\n",
    "\n",
    "#Function for text cleansing\n",
    "def cleansing(text):\n",
    "    text = lowercase(text)\n",
    "    text = special(text)\n",
    "    text = single(text)\n",
    "    text = singlestart(text)\n",
    "    text = mulspace(text)\n",
    "    text = rt(text)\n",
    "    text = prefixedb(text)\n",
    "    text = misc(text)\n",
    "    text = alay(text)\n",
    "    text = stopwrds(text)\n",
    "    return text\n",
    "\n",
    "#Sklearn Neural Network Analysis Sentiment\n",
    "#Load the sklearn Model\n",
    "f1 = joblib.load('data/score.pkl')\n",
    "clf = joblib.load('data/model.pkl')\n",
    "vectorizer = joblib.load('data/vectorizer.pkl')\n",
    "\n",
    "#Function for CSV Sklearn Analysis\n",
    "def sentiment_csv_nn(input_file):\n",
    "    column = input_file.iloc[:, 0]\n",
    "    print(column)\n",
    "    \n",
    "    #Define and execute query for insert cleaned text and sentiment to sqlite database\n",
    "    for data_file in column:\n",
    "        data_clean = cleansing(data_file)\n",
    "        sent = clf.predict(vectorizer.transform([data_clean]).toarray())\n",
    "        query = \"insert into data_file_sk ('Text', 'Sentiment') values (?, ?)\"\n",
    "        val = (data_clean,str(sent))\n",
    "        conn.execute(query, val)\n",
    "        conn.commit()\n",
    "        print(data_file)\n",
    "\n",
    "#Create Homepage\n",
    "@swag_from(r\"C:\\Users\\rahad\\Downloads\\Platinum Binar Academy\\docs\\welcome_pages.yml\", methods=['GET'])\n",
    "@app.route('/', methods=['GET'])\n",
    "def get():\n",
    "    return \"Welcome to Sental Dashboard\"\n",
    "\n",
    "#Text Analysis Sklearn\n",
    "#Input text to analyze\n",
    "@swag_from(r\"C:\\Users\\rahad\\Downloads\\Platinum Binar Academy\\docs\\text_sklearn.yml\", methods=['POST'])\n",
    "@app.route('/text_sklearn', methods=['POST'])\n",
    "def text_sentiment_sklearn():\n",
    "    #Get text from user\n",
    "    input_text = str(request.form['text'])\n",
    "    \n",
    "    #Cleaning text\n",
    "    output_text = cleansing(input_text)\n",
    "    \n",
    "    #Model Prediction for Sentiment Analysis\n",
    "    sent = clf.predict(vectorizer.transform([output_text]).toarray())\n",
    "    \n",
    "    # Define and execute query for insert cleaned text and sentiment to sqlite database\n",
    "    query = \"insert into data_text_sk (text,sentiment) values (?, ?)\"\n",
    "    val = (output_text,str(sent))\n",
    "    conn.execute(query, val)\n",
    "    conn.commit()\n",
    "    \n",
    "    #Define API Response\n",
    "    json_response = {\n",
    "        'description': \"Analysis Sentiment Success!\",\n",
    "        'F1 on test set': f1,\n",
    "        'text' : output_text,\n",
    "        'sentiment' : str(sent)\n",
    "    }\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "#Endpoint for File Analysis SKLearn\n",
    "@swag_from(r\"C:\\Users\\rahad\\Downloads\\Platinum Binar Academy\\docs\\file_sklearn.yml\", methods=['POST'])\n",
    "@app.route('/file_sklearn', methods=['POST'])\n",
    "def file_sentiment_sk():\n",
    "    #Get File\n",
    "    file = request.files['file']\n",
    "    try:\n",
    "            datacsv = pd.read_csv(file, encoding='iso-8859-1')\n",
    "    except:\n",
    "            datacsv = pd.read_csv(file, encoding='utf-8')\n",
    "    \n",
    "    #Cleaning file\n",
    "    sentiment_csv_nn(datacsv)\n",
    "    \n",
    "    #Define API response\n",
    "    select_data = conn.execute(\"SELECT * FROM data_file_sk\")\n",
    "    conn.commit\n",
    "    data = [\n",
    "        dict(text_id=row[0], text=row[1], sentiment=row[2])\n",
    "    for row in select_data.fetchall()\n",
    "    ]\n",
    "    \n",
    "    return jsonify(data)\n",
    "\n",
    "#Tensorflow LSTM Model Analysis Sentimen\n",
    "#Load the Tensorflow Model\n",
    "model = load_model(r\"C:\\Users\\rahad\\Downloads\\Platinum Binar Academy\\data\\LSTM_Binar_model.h5\")\n",
    "tokenizer = joblib.load(r\"C:\\Users\\rahad\\Downloads\\Platinum Binar Academy\\data\\tokenizer.pickle\")\n",
    "\n",
    "#Model Prediction\n",
    "#Create Function for Sentiment Prediction\n",
    "def predict_sentiment(text):\n",
    "    sentiment_tf = ['negative', 'neutral', 'positive']\n",
    "    text = cleansing(text)\n",
    "    tw = tokenizer.texts_to_sequences([text])\n",
    "    tw = pad_sequences(tw, maxlen=96)\n",
    "    prediction = model.predict(tw)\n",
    "    polarity = np.argmax(prediction[0])\n",
    "    return sentiment_tf[polarity]\n",
    "\n",
    "def sentiment_csv_tf(input_file):\n",
    "    column = input_file.iloc[:, 0]\n",
    "    print(column)\n",
    "    \n",
    "    # Define and execute query for insert cleaned text and sentiment to sqlite database\n",
    "    for data_file in column:\n",
    "        data_clean = cleansing(data_file)\n",
    "        sent = predict_sentiment(data_clean)\n",
    "        query = \"insert into data_file_tf ('Text', 'Sentiment') values (?, ?)\"\n",
    "        val = (data_clean,sent)\n",
    "        conn.execute(query, val)\n",
    "        conn.commit()\n",
    "        print(data_file)\n",
    "\n",
    "#Endpoint for Text Analysis Tensorflow\n",
    "#Input text to analyze\n",
    "@swag_from(r\"C:\\Users\\rahad\\Downloads\\Platinum Binar Academy\\docs\\text_tensorflow.yml\", methods=['POST'])\n",
    "@app.route('/text_tensorflow', methods=['POST'])\n",
    "def text_sentiment_tf():\n",
    "    #Get text from user\n",
    "    input_text = str(request.form['text'])\n",
    "    \n",
    "    #Cleansing text\n",
    "    output_text = cleansing(input_text)\n",
    "    \n",
    "    #Model Prediction for Sentiment Analysis\n",
    "    output_sent = predict_sentiment(output_text)\n",
    "    \n",
    "    #Define and execute query for insert cleaned text and sentiment to sqlite database\n",
    "    query = \"insert into data_text_tf (Text,Sentiment) values (?, ?)\"\n",
    "    val = (output_text,output_sent)\n",
    "    conn.execute(query, val)\n",
    "    conn.commit()\n",
    "    \n",
    "    #Define API response\n",
    "    json_response = {\n",
    "        'description': \"Analysis Sentiment Success!\",\n",
    "        'text' : output_text,\n",
    "        'sentiment' : output_sent\n",
    "    }\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "#Endpoint for File Analysis Tensorflow\n",
    "@swag_from(r\"C:\\Users\\rahad\\Downloads\\Platinum Binar Academy\\docs\\file_tensorflow.yml\", methods=['POST'])\n",
    "@app.route('/file_tensorflow', methods=['POST'])\n",
    "def file_sentiment_tf():\n",
    "    #Get file\n",
    "    file = request.files['file']\n",
    "    try:\n",
    "            datacsv = pd.read_csv(file, encoding='iso-8859-1')\n",
    "    except:\n",
    "            datacsv = pd.read_csv(file, encoding='utf-8')\n",
    "    \n",
    "    #Cleaning file\n",
    "    sentiment_csv_tf(datacsv)\n",
    "    \n",
    "    #Define API response\n",
    "    select_data = conn.execute(\"SELECT * FROM data_file_tf\")\n",
    "    conn.commit\n",
    "    data = [\n",
    "        dict(text_id=row[0], text=row[1], sentiment=row[2])\n",
    "    for row in select_data.fetchall()\n",
    "    ]\n",
    "    \n",
    "    return jsonify(data)\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b5e4ea6367dabdbececb40e29ae3e35f2cf9fc411d4c5b5c39380433476f7338"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
